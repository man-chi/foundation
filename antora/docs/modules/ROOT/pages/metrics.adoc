= Metrics
:navtitle: metrics

== Developer Experience (DX)

Metric 1: Onboarding time::
* Time to first commit to production: From their first day, how long does it take a new developer to commit code that goes to production?
* Velocities of other teammates: You might also notice that a team’s velocity slows down when it takes on a new teammate.

Metric 2: Inner loop optimization::
* Time to first file open: How long does a developer wait after opening their IDE before they can start editing a file?
* Build time: How long does it take to build a project?

Metric 3: Bug resolution time::
* Time between defect detection and resolution: This is a simplistic metric, to be sure. Still, a team with a decelerating bug resolution time is a sign of trouble.
* Bug severity distribution, bug assignments, bug reporting frequency: These metrics give clues about the quality of your code. A disproportionate number of high-priority bugs is a signal of poor code quality. Or it might be a sign of the quality of the bug reports themselves. Are bugs clearly reported and reproducible? Do they contain sufficient information for a developer to work from?
* Defect resolution efficiency (DRE): To pinpoint which part of your process is causing more defects, consider DRE. It’s a more sophisticated set of metrics than just time between detection and resolution. DRE measures the percentage of defect resolutions between different phases of development. DRE is a good prevention tool: the sooner you find and resolve a defect, the less it costs your organization.

Metric 4: Code reviews::
* Response time for code reviews: How long does it take before a teammate responds to a PR? Does your team’s workflow allow a developer to work on other issues while waiting for a PR? How do team leads, managers, or product owners manage assignments for code reviews?
* Number of comments per review, ratio of accepted and rejected code changes: These metrics give clues to the quality of the reviews.

Metric 5: Deployment frequency::
* Deployment frequency: This is an obvious one, but it still helps to understand the nature of your deployments. What is the historical average time to deploy? For example, can your team promise monthly deployments if it averages 12 deployments in a year? Or can it deploy on the 1st of every month for a year?
* Success/failure rate of deployments: How often are deployments rolled back or re-deployed because of an incident? This is another obvious one, but it gives a clue to how well your teams are collaborating and detects early signs of weaknesses.

== The important metrics defined by DORA (DevOps Research and Assessment) include:
* R: Mean Time to Restore (MTTR)
* E: Change Failure Rate
* L: Lead Time for Changes
* F: Deployment Frequency

reference::
https://coder.com/blog/elevating-developer-experience[developer experience]
